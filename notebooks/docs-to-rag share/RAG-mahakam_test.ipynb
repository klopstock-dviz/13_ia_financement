{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b419c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 970aa74c0a90: 100% ▕██████████████████▏ 274 MB                         \u001b[K\n",
      "pulling c71d239df917: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling ce4a164fc046: 100% ▕██████████████████▏   17 B                         \u001b[K\n",
      "pulling 31df23ea7daa: 100% ▕██████████████████▏  420 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# créer un nouvel env conda à partir du terminal\n",
    "# conda create --name pathrag python=3.10\n",
    "# installer ollama: https://www.ollama.com/download\n",
    "\n",
    "# installer les dépendences\n",
    "#%pip install -r requirements.txt\n",
    "# intsaller le modème d'OllamaEmbeddings\n",
    "!ollama pull nomic-embed-text\n",
    "# créer une clé api sur openrouter pour utiliser des llm gratuitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0299e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chougar/miniconda3/envs/pathrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "from pathrag_retriever import create_graphdb, load_existing_graphdb, load_knowledgeGraph_vis\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573138d",
   "metadata": {},
   "source": [
    "#### Base vectorielle\n",
    "Création de la base vect. avec le PP Mahakam (20 premières pages, ligne 45 `docs[:20]`)\n",
    "\n",
    "Relancer la cellule à chaque ouverture pour utiliser le rag vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514d8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 533 documents from PU_P01_PP01.docx\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "ERROR: Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR: Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "ERROR: Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "#========= choix du modèle d'embedding\n",
    "\"\"\"\n",
    "    Le modèle choisi impacte la qualité du retriever, mais aussi le temps de traitement\n",
    "    Si le déploiement est prévu sur une VM limitée, un modèle plus petit est nécessaire\n",
    "    Explorer les comparatifs: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\"\"\"\n",
    "# Utiliser OllamaEmbeddings avec le modèle local \"nomic-embed-text\"\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "\n",
    "\n",
    "# chargement et fragmentation du doc\n",
    "filename=\"PU_P01_PP01.docx\"\n",
    "doc_name_hybrid=\"PP mahakam light\" # nom de doc significatif\n",
    "\n",
    "\n",
    "# loader = UnstructuredFileLoader(filename)\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "docx_docs = loader.load()\n",
    "print(f\"Loaded {len(docx_docs)} documents from {filename}\")\n",
    "\n",
    "\n",
    "#======== choix des paramètres de fragmentation\n",
    "\"\"\"\n",
    "    la taille du chunck_size est très important dans l'accès à une info précise\n",
    "    une plus petite taille permet de cibler de courts passages contenant l'info nécessaire à des réponses précises:\n",
    "        * lieu du projet\n",
    "        * dates du projet\n",
    "        * budget ...    \n",
    "    l'envoi de passages plus courts au llm évite une dispertion de son attention\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(docx_docs)\n",
    "\n",
    "# Filter out complex metadata (e.g., lists, dicts)\n",
    "docs = [Document(doc.page_content) for doc in docs[:20]]\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "# Conversion des docs en embeddings \n",
    "chroma_db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=f'./storage/vector_scores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "    collection_name=doc_name_hybrid.replace(\" \",\"_\")\n",
    ")\n",
    "\n",
    "retriever=chroma_db.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code...\n",
    "all_docs = chroma_db.get()\n",
    "print(len(all_docs['documents']))  # This will print the total number of docs stored\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbb31f",
   "metadata": {},
   "source": [
    "#### Création ou chargement d'un graphe existant:\n",
    "Saisir l'action désirée dans le prompt qui s'affiche en haut du notebook, suivre instructions\n",
    "\n",
    "Par défaut prise en compte des 20 premières pages pour l'exemple, modifier ligne 11 `for doc in docx_docs[:20]:`\n",
    "\n",
    "Si vous voulez charger un graphe déjà crée, retrouver son nom dans le fichier `graphrag_hashes.json`, attribut `Nom du doc`\n",
    "\n",
    "Si vous voulez modifier le LLM utilisé pour la création du graphe ou sa lecture, allez dans `pathrag_retriever.py`, ligne 34 et 35, et prenez un modèle valide sur openrouter (attention à prendre un modèle qui supporte les `structured_outputs`, à filtrer à droite dans la liste des `Supported parameters`)\n",
    "\n",
    "**Important**: renseigner votre clé api openrouter sur le script `pathrag_retriever.py`, ligne 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63b4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PathRAG:Logger initialized for working directory: /home/chougar/Documents/GitHub/Projet-portail-immo/docs-to-rag share/storage/graph_stores/6eb62bb393f33532a59e24b774045344e61fdc93747de585e719bc100467d2a0\n",
      "INFO:PathRAG:Load KV llm_response_cache with 0 data\n",
      "INFO:PathRAG:Load KV full_docs with 1 data\n",
      "INFO:PathRAG:Load KV text_chunks with 1 data\n",
      "INFO:PathRAG:Loaded graph from /home/chougar/Documents/GitHub/Projet-portail-immo/docs-to-rag share/storage/graph_stores/6eb62bb393f33532a59e24b774045344e61fdc93747de585e719bc100467d2a0/graph_chunk_entity_relation.graphml with 33 nodes, 16 edges\n",
      "INFO:nano-vectordb:Load (32, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/Projet-portail-immo/docs-to-rag share/storage/graph_stores/6eb62bb393f33532a59e24b774045344e61fdc93747de585e719bc100467d2a0/vdb_entities.json'} 32 data\n",
      "INFO:nano-vectordb:Load (16, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/Projet-portail-immo/docs-to-rag share/storage/graph_stores/6eb62bb393f33532a59e24b774045344e61fdc93747de585e719bc100467d2a0/vdb_relationships.json'} 16 data\n",
      "INFO:nano-vectordb:Load (1, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/home/chougar/Documents/GitHub/Projet-portail-immo/docs-to-rag share/storage/graph_stores/6eb62bb393f33532a59e24b774045344e61fdc93747de585e719bc100467d2a0/vdb_chunks.json'} 1 data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nom de votre graphe est PP mahakam light\n",
      "\n",
      "        ----------------\n",
      "        #### Graph RAG retriever\n",
      "        Chargement de la base Graph RAG\n",
      "    \n",
      "**✅ Graph RAG chargé**\n"
     ]
    }
   ],
   "source": [
    "# appliquer nest_asyncio uniquement sur notebook pour corriger l'erreur de loop event\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# remetre à plat le text\n",
    "filename=\"PU_P01_PP01.docx\"\n",
    "loader = UnstructuredLoader(filename)\n",
    "\n",
    "docx_docs = loader.load()\n",
    "text=\"\"\n",
    "for doc in docx_docs[:20]:\n",
    "    text+=doc.page_content\n",
    "\n",
    "\n",
    "r=input(\"Saisir 'C' pour créer un nouveau graphe, 'L' pour charger un graphe existant\")\n",
    "\n",
    "# créer un nouveau graphe\n",
    "messages=None\n",
    "if r=='C':\n",
    "    doc_name_graph=input('Saisir un nom unique pour votre graphe')\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "    messages= create_graphdb(\n",
    "        text=text, \n",
    "        doc_name=doc_name_graph, # il faut donner un nom unique permettant d'identifier et charger le graph les prochaines fois\n",
    "    )\n",
    "# charger un graphe existant\n",
    "elif r=='L':\n",
    "    doc_name_graph=input('Saisir le nom du graphe à charger')\n",
    "    print(f\"Le nom de votre graphe est {doc_name_graph}\")\n",
    "\n",
    "    messages=load_existing_graphdb(doc_name_graph)\n",
    "else:\n",
    "    print('Option invalide')\n",
    "\n",
    "\n",
    "\n",
    "if messages:\n",
    "    pipeline_args={}\n",
    "    for feedback in messages:\n",
    "        if isinstance(feedback, str):\n",
    "            print(feedback)\n",
    "        elif isinstance(feedback, dict):\n",
    "            pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"]=feedback[\"pipeline_args\"]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6de14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:PathRAG:kw_prompt result:\n",
      "INFO:PathRAG:```json\n",
      "{\n",
      "  \"high_level_keywords\": [\"Analyse textuelle\", \"Thèmes principaux\", \"Questions de compréhension\"],\n",
      "  \"low_level_keywords\": [\"Identification des thèmes\", \"Formulation de questions\", \"Lecture analytique\", \"Compréhension de texte\"]\n",
      "}\n",
      "```\n",
      "INFO:PathRAG:Local query uses 32 entites, 15 relations, 1 text units\n",
      "INFO:PathRAG:Global query uses 20 entites, 16 relations, 1 text units\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response all ready\n",
      "\n",
      "\n",
      "## Principaux thèmes et questions potentielles du texte\n",
      "\n",
      "Le texte présente un projet de restauration de mangroves dans l’est de Kalimantan, en Indonésie, et aborde plusieurs thèmes clés interconnectés.  Voici une analyse des principaux thèmes et des questions qu’ils soulèvent :\n",
      "\n",
      "**1. Dégradation de l'environnement et restauration des écosystèmes :**\n",
      "\n",
      "*   **Description :** Un thème central est la dégradation des écosystèmes de mangroves dans le delta du Mahakam et la baie d'Adang en raison de l'exploitation excessive des terres pour l'aquaculture. Le projet se concentre sur la restauration de ces mangroves dégradées.\n",
      "*   **Questions potentielles :**\n",
      "    *   Quels sont les impacts spécifiques de la dégradation des mangroves sur l'environnement local et les communautés ?\n",
      "    *   Quelles sont les techniques de restauration des mangroves qui seront utilisées dans le projet ?\n",
      "    *   Existe-t-il des conflits d'intérêts entre les activités d'aquaculture et la restauration des mangroves, et comment le projet entend-il les gérer ?\n",
      "    *   Quel est l'état initial de la mangrove, et comment le succès de la restauration sera-t-il mesuré ?\n",
      "\n",
      "**2. Changement climatique et atténuation :**\n",
      "\n",
      "*   **Description :** Le texte souligne le rôle crucial des mangroves dans la séquestration du carbone et la lutte contre le changement climatique, ce qui constitue une motivation importante pour la restauration.\n",
      "*   **Questions potentielles :**\n",
      "    *   Quelle quantité de carbone les mangroves restaurées pourraient-elles séquestrer ?\n",
      "    *   Comment le projet contribue-t-il à l'adaptation des communautés locales aux impacts du changement climatique ?\n",
      "    *   Le projet intègre-t-il des stratégies pour renforcer la résilience des mangroves face aux événements climatiques extrêmes ?\n",
      "\n",
      "**3. Développement durable et Objectifs de développement durable (ODD) :**\n",
      "\n",
      "*   **Description :** Le projet est explicitement aligné sur plusieurs ODD, notamment la lutte contre la pauvreté, la protection de l'environnement et la promotion d'une gestion durable des ressources naturelles. Il vise également à soutenir des moyens de subsistance durables pour les populations locales.\n",
      "*   **Questions potentielles :**\n",
      "    *   Quels sont les ODD spécifiques auxquels le projet contribue le plus directement ?\n",
      "    *   Comment le projet implique-t-il les communautés locales dans la planification et la mise en œuvre des activités ?\n",
      "    *   Quels sont les bénéfices socio-économiques attendus du projet pour les communautés locales ?\n",
      "    *   Comment le projet assure-t-il l’équité et l’inclusion dans la distribution des bénéfices ?\n",
      "\n",
      "**4. Gouvernance et Collaboration :**\n",
      "\n",
      "*   **Description :** Le projet implique une collaboration entre plusieurs organisations, notamment Planète Urgence, Yayasan Planete Urgensi Indonesia et POKJA. La gouvernance côtière est également un aspect important.\n",
      "*   **Questions potentielles :**\n",
      "    *   Quel est le rôle spécifique de chaque organisation impliquée dans le projet ?\n",
      "    *   Comment la collaboration entre les différentes organisations est-elle structurée et gérée ?\n",
      "    *   Comment le projet renforce-t-il la gouvernance côtière et la participation des parties prenantes locales ?\n",
      "\n",
      "**5. Relocalisation de la capitale et impacts régionaux :**\n",
      "\n",
      "*   **Description :** Le projet prend en compte le contexte plus large du déplacement de la capitale indonésienne de Jakarta vers l'est de Kalimantan, et prévoit une pression accrue sur les ressources naturelles et l'environnement de la région.\n",
      "*   **Questions potentielles :**\n",
      "    *   Comment le transfert de la capitale pourrait-il affecter la dégradation des mangroves et les activités d'aquaculture ?\n",
      "    *   Le projet tient-il compte des plans d'aménagement de la nouvelle capitale et des infrastructures associées ?\n",
      "    *   Quels sont les mécanismes de coordination entre le projet et les autorités gouvernementales responsables du transfert de la capitale ?\n",
      "\n",
      "**6. Biodiversité et Espèces Menacées :**\n",
      "\n",
      "*   **Description :** L'habitat des mangroves héberge des espèces menacées, comme le nasique, et le projet vise à protéger la biodiversité.\n",
      "*   **Questions Potentielles :**\n",
      "    *   Quels sont les impacts spécifiques de la dégradation des mangroves sur la population de nasiques ?\n",
      "    *   Comment le projet contribuera-t-il à la protection de la biodiversité dans la région ?\n",
      "    *   Existe-t-il d'autres espèces menacées qui pourraient bénéficier des activités de restauration des mangroves ?\n",
      "\n",
      "\n",
      "\n",
      "En résumé, le texte présente un projet complexe et multidimensionnel qui aborde des questions environnementales, sociales et économiques cruciales. Les questions énumérées ci-dessus sont un point de départ pour une analyse plus approfondie des enjeux et des perspectives du projet."
     ]
    }
   ],
   "source": [
    "from PathRAG import QueryParam\n",
    "import asyncio\n",
    "\n",
    "def stream_pathRAG_response(stream_resp):\n",
    "    async def stream_response():        \n",
    "        # Process the async generator\n",
    "        async for chunk in stream_resp:\n",
    "            print(chunk or \"\", end=\"\")\n",
    "\n",
    "\n",
    "    # Run in Streamlit's existing event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(stream_response())\n",
    "\n",
    "# question=\"résume ce texte dans sa langue source\"\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "\n",
    "resp=pipeline_args[f\"graphrag_pipeline_{doc_name_graph}\"][\"rag\"].query(query= question, param=QueryParam(mode=\"hybrid\", stream=True))\n",
    "\n",
    "stream_pathRAG_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f595f8",
   "metadata": {},
   "source": [
    "Renseigner votre clé d'api sur la ligne 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84137027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.schema.document import Document\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "\n",
    "class RAG_hybrid():\n",
    "    def __init__(self, model):\n",
    "        self.model=model\n",
    "        self.retrieved_docs=[]\n",
    "        self.semantic_retriever_topK=10\n",
    "        self.sparse_retriever_topK=10\n",
    "        self.history=[]\n",
    "        self.llm_client = AsyncOpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=\"\",\n",
    "        )\n",
    "        self.reranker_llm=\"mistralai/mistral-small-3.1-24b-instruct:free\"\n",
    "        self.reranker_score_thresh=5\n",
    "        self.reranked_doc=[]\n",
    "\n",
    "    def semanticRetriever(self):\n",
    "        # 1. Semantic Retriever (Chroma + OllamaEmbeddings)\n",
    "        embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        chroma_db = Chroma(\n",
    "            persist_directory=f'./storage/vector_scores/{doc_name_hybrid.replace(\" \",\"_\")}',\n",
    "            collection_name=doc_name_hybrid.replace(\" \",\"_\"),\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "\n",
    "        semantic_retriever=chroma_db.as_retriever(search_type=\"mmr\", k=self.semantic_retriever_topK)\n",
    "\n",
    "        self.chroma_db=chroma_db\n",
    "        self.semantic_retriever=semantic_retriever\n",
    "    \n",
    "    def sparseRetriever(self):\n",
    "        # 2. Sparse Retriever (TF-IDF)\n",
    "\n",
    "        # Récupérer TOUS les documents depuis Chroma\n",
    "        all_data = self.chroma_db.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "        # Convertir en liste de `Document` objects pour LangChain\n",
    "        docs = [\n",
    "            Document(page_content=text, metadata=meta or {})  # <-- Si meta est None, on met {}\n",
    "            for text, meta in zip(all_data[\"documents\"], all_data[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        # Créer le retriever TF-IDF\n",
    "        sparse_retriever = TFIDFRetriever.from_documents(\n",
    "            documents=docs,\n",
    "            k=self.sparse_retriever_topK,\n",
    "            tfidf_params={\"min_df\": 1, \"ngram_range\": (1, 2)}\n",
    "        )\n",
    "\n",
    "        self.sparse_retriever= sparse_retriever\n",
    "    \n",
    "    def ensembleRetriever(self):\n",
    "        # 3. Ensemble Retriever (Semantic + Sparse)\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.semantic_retriever, self.sparse_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever=ensemble_retriever\n",
    "\n",
    "    async def reranker(self, results, query):\n",
    "\n",
    "\n",
    "        async def llm_eval(doc, query):\n",
    "            system_prompt=\"\"\"\n",
    "                You're an expert assistant in reranking documents against a question.\n",
    "                Your role is to compare the question with a document and give a score from 0 to 10, where:\n",
    "                0=document out of context, unable to answer the question\n",
    "                10=highly relevant document, able to answer the question\n",
    "                                \n",
    "                The expected final output is the score in json format\n",
    "                Example:\n",
    "                ```json{\"score\": 5}```\n",
    "                \n",
    "                Always end your answer with this format                \n",
    "            \"\"\"            \n",
    "            response = await self.llm_client.chat.completions.create(\n",
    "                model=self.reranker_llm,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"La question est: {query}\\n Le document à évaluer est le suivant\\n: {doc}\" }\n",
    "                ],\n",
    "                temperature=0,\n",
    "            )\n",
    "            # Post-process to extract only the JSON part if extra text is present\n",
    "            content = response.choices[0].message.content\n",
    "            # Try to extract the JSON block if the model adds extra text\n",
    "            match = re.search(r\"\\{.*?\\}\", content, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "\n",
    "            # extract score\n",
    "            score=None\n",
    "            try:\n",
    "                score=content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                score= json.loads(score)\n",
    "                score=score[\"score\"]\n",
    "            except Exception as e:\n",
    "                print(e)                \n",
    "            \n",
    "            return {\"content\": doc, \"score\": score}\n",
    "\n",
    "\n",
    "        tasks=[llm_eval(doc.page_content, query) for doc in results]\n",
    "        scored_docs= await asyncio.gather(*tasks)\n",
    "        i=1\n",
    "\n",
    "        for doc in scored_docs:\n",
    "          \n",
    "            print(f'chunk {i} score: {doc[\"score\"]}')\n",
    "            i+=1\n",
    "\n",
    "        filtred_docs=[d for d in scored_docs if d[\"score\"]>=self.reranker_score_thresh]\n",
    "        # print(f\"scored docs; \\n{scored_docs}\")\n",
    "        self.reranked_doc=filtred_docs\n",
    "\n",
    "        return filtred_docs\n",
    "\n",
    "    async def ask_llm(self, query):\n",
    "        # 5. Final processing step with an LLM (e.g., OpenAI via OpenRouter)\n",
    "\n",
    "        # init retrievers\n",
    "        self.semanticRetriever()\n",
    "        self.sparseRetriever()\n",
    "        self.ensembleRetriever()\n",
    "\n",
    "        # retrieve relevant docs\n",
    "        results = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        print(f\"Nb of retrieved docs: {len(results)}\")\n",
    "\n",
    "        # rerank\n",
    "        scored_results=await self.reranker(results, query)\n",
    "        \n",
    "        # Concatenate retrieved documents for context\n",
    "        context = \"\\n\".join([f\"Fragment: \\n{doc['content']}\\n\" for doc in scored_results])\n",
    "\n",
    "        print(f\"Context lenght: {len(context.split(' '))} words\")\n",
    "        llm_prompt = f\"\"\"\n",
    "            Answer the question based **only** on the provided context.  \n",
    "\n",
    "            - If the context contains enough information to provide a complete or partial answer, use it to formulate a detailed and factual response.  \n",
    "            - If the context lacks relevant information, respond with: \"I don't know.\"  \n",
    "\n",
    "            ### **Context:**  \n",
    "            {context}  \n",
    "\n",
    "            ### **Question:**  \n",
    "            {query}  \n",
    "\n",
    "            ### **Answer:**  \n",
    "            Provide a clear, factual, and well-structured response based on the available context. Avoid speculation or adding external knowledge.  \n",
    "        \"\"\"\n",
    "\n",
    "        llm_completion = await self.llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in document Q/A and document synthesis\"},\n",
    "                {\"role\": \"user\", \"content\": llm_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        final_answer = \"\"\n",
    "        print(\"Réponse:\\n=========\")\n",
    "        async for chunk in llm_completion:\n",
    "            if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "                final_answer += chunk.choices[0].delta.content\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        \n",
    "        self.history+=[\n",
    "            {\"role\": \"user\", 'content': query},\n",
    "            {\"role\": \"assistant\", \"content\": final_answer}\n",
    "        ]\n",
    "        \n",
    "        return final_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a224490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR: Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of retrieved docs: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 score: 3\n",
      "chunk 2 score: 7\n",
      "chunk 3 score: 0\n",
      "chunk 4 score: 3\n",
      "chunk 5 score: 5\n",
      "chunk 6 score: 0\n",
      "chunk 7 score: 3\n",
      "chunk 8 score: 6\n",
      "chunk 9 score: 9\n",
      "chunk 10 score: 4\n",
      "chunk 11 score: 3\n",
      "chunk 12 score: 2\n",
      "Context lenght: 181 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse:\n",
      "=========\n",
      "Les principaux thèmes abordés dans le texte sont :\n",
      "\n",
      "1. **Conservation des écosystèmes** : Le texte mentionne les efforts de conservation menés par différentes parties et le gouvernement pour protéger l'écosystème de la Delta Mahakam et de la baie d'Adang.\n",
      "\n",
      "2. **Déforestation et dégradation des mangroves** : Il est question de la dégradation des mangroves due à la conversion des terres en zones d'aquaculture, avec une mention spécifique de 47,5 % de l'écosystème mangrove dégradé en 2017.\n",
      "\n",
      "3. **Espèces menacées** : Le texte souligne la présence d'espèces en danger critique d'extinction, comme le nasique (proboscis monkey), endémique à l'île de Bornéo.\n",
      "\n",
      "4. **Changement climatique** : Les mangroves sont décrites comme un écosystème clé pour la séquestration du carbone et la lutte contre le changement climatique.\n",
      "\n",
      "5. **Gestion des terres** : Il est fait référence à la propriété des terres dans la Delta Mahakam, qui est une terre gouvernementale désignée comme une forêt de production, mais habitée par des résidents de génération en génération.\n",
      "\n",
      "### Questions qui peuvent être posées :\n",
      "\n",
      "1. Quelles sont les principales causes de la dégradation des mangroves dans la Delta Mahakam ?\n",
      "2. Quelles espèces menacées sont présentes dans la Delta Mahakam et la baie d'Adang ?\n",
      "3. Quels sont les efforts de conservation en cours pour protéger ces écosystèmes ?\n",
      "4. Comment les mangroves contribuent-elles à la lutte contre le changement climatique ?\n",
      "5. Quelle est la situation de la propriété des terres dans la Delta Mahakam et comment cela affecte-t-il la conservation des mangroves ?\n",
      "6. Quelles sont les conséquences de la conversion des mangroves en zones d'aquaculture ?\n",
      "7. Quels sont les impacts de la déforestation sur les espèces endémiques comme le nasique ?"
     ]
    }
   ],
   "source": [
    "rag_hybrid=RAG_hybrid(model=\"mistralai/mistral-small-3.2-24b-instruct:free\")\n",
    "# 4. Ask a question\n",
    "question = \"Quels sont les principaux thèmes de ce texte et les questions qui peuvent être posées ?\"\n",
    "results = await rag_hybrid.ask_llm(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
