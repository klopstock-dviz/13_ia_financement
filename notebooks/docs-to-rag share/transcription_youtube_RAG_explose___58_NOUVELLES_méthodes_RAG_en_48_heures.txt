Title: RAG explose : 58 NOUVELLES méthodes RAG en 48 heures
Description: 28 k vues  il y a 2 semaines  #mcp #emergence #scienceexplained
Guide Hitchhackers de la galaxie RAG : Un tutoriel pour débutants et experts.

Le RAG 2025 expliqué : D'un système RAG classique à un graphe de connaissances multi-agents RAG RL en 2025. …
...afficher plus


Transcription :

hello community Let's talk about rack and I mean rack today May 21st 2025
Let's explain it Well of course we could start here immediately with a chain based knowledge graph rack methodology
or we go with a multi- aent rag with a semantic partitioning of linked information for type specialized split
rag But you know what it is amazing in the last two days my AI deep research
detected here 58 new AI research preprint This means new rack implementation in two days Those are the
two days and we will have a look about one or two dozens of them But let's start because I was also asked hey if I
start with rack today can you explain it Well of course I'm so glad that you ask
Rag explained Now in the good old times of artificial intelligence you know we only had a
single monolytic large language model to do both here the search and the generator and the augmentation And this
is here what we call drag in the good old times when humanity and the eye just invented here the wheel and where were
about it stone age No And then we found out like for example here with open eyes
proprietary 03 pro we have a very knowledgeable expert LLM who can answer
question well if given the right information from external data sources and we developed quite a lot of new
function function calling we have you know MCP A2A model context protocols
agents to agent protocols but you know it's not really a good performance so what we did we said okay But this LLM
here this expert LLM doesn't really know how to find information efficiently
because maybe it doesn't compose here the complexity in an efficient way Maybe it has no feedback loop Give me a
particular search engine and there are a lot of other reasons So naturally what
we did we wanted to train here a separate LLM we call it now a search or
a searcher LLM whose only job and absolutely now trained to do the find
the best possible documents for the other LLM So we have now a specific tool
that this system is optimized for and let's say it's a search engine and go with the most famous one Let's do with
Google So you see suddenly we had two or multiple LLMs and you know what's
happening now in the last month we went from an LLM to an agent now we have
multiple agent we have multi- aent system training so we are now if you want now in the renaissance here 500
years before today as you see it's not anymore rag
but let's go because it's such a beautiful old term no rag so the retriever is not anymore there a
retriever if you want to call it Because now we have a specifically trained a dedicated EI agent maybe even a cluster
of ERI agent more about this later with some excellent tool use capabilities We
have a self-arning agent for those tool users that is extensively trained on a specific search engine So this is a
highly dedicated expert system that is really improving the quality of our
complete rack system But you see from the good old retriever days no when we had I don't know lang chain or something
like this this is all gone this is not anymore there because now we have a complete different architecture of it
but rack the term in itself is still there for external data for an
artificial intelligence there was one paper in 2025
that changed everything and this was here our deepseek R1 and I don't know if you noticed this especially the R1 zero
model because this showed us here in the community that even rule-based outcomedriven rewards can train strong
reasoning agents and this kicked off something amazing Let me show you this
paper I don't know if you're familiar This is University of Illinois and Korea University the deep retrieval So the
real search engines and retrieval with LLMs via reinforcement learning
This was quite an influential paper because it had a reinforcement learning approach to train our very dedicated
LLMs for a very generation true not human annotated but trial and error
without supervised data This was amazing You just tell a system hey self optimize
yourself And of course we had to have success factors We call it here retrieval metrics here And if you know
about reinforcement learning we have there another term we call it a reward function or reward
system If we define now this metric this rewards our system generated queries
that really maximize here the retrieval performance Great So this was here 2025
absolutely amazing Let's have here a view abstraction here So we have here the
user query No this is the input No this is great But you know a user query is not really precise It's not to the point
It's not optimized for a specific search engine It's not optimized here for the format maybe you have in your
proprietary database in your company or not optimized for a particular NoSQL
databases So if you go to the internet you have to have a multitude of different query structures So what do
you do you have here LLM and you do the reasoning and said hey let's augment now this human query so you have a thinking
process and then you say okay he have now optimized the query you have an augmented query or you have multiple
augmented query that you have now multiple parallel augmented query that are now the real if you want input to
the retrieval to the search engine let's say it's Google or you have a SQL nosql whatever you like never
mind and then you had somewhere the true context and you retrieved here from the true context the retrieved context given
here your particular augmented query and you have a retrieval reward computation
It is simply you check hey given here my augmented query is this really the content that would answer this and you
have also different reward function like a final reward function includes here a format reward not just the retrieval
reward and you can calculate this and you see hey some of those things are
really working great and some of have no no generated information at all so you
update here with the specific parameter now your LLM So you see we have now if you want a
looping process to optimize you the LLM specifically in real time for this
particular search engine or for your particular SQL whatever you have So we
have here a dedicated optimization problem in front of us So you see the rack here the retrieve
is not there anymore the augmentation from a monolithic LLM is not anymore there because what we have now is a
reinforcement learning approach that trains now this let's say we know it's an agent of let's say it's a large
language model for the very generation itself and this is really great and they
did some tests on the performance on the benchmark and they tell us here it outperforms here the leading methods on
literature search if you are science you know literature search with 65% and soda
is 24% recall for publication search So this is a big jump So this is something
where we say okay if we let the system train to optimize itself not the human
search query but hey let it search and let it optimize
itself This is where AI shines If you want to see this by the way yeah I'm
always ask about code here we have the deep retrieval here the GitHub repo Everything is available for you there
However if we now go a step further do you notice that there is something not
really optimal not really perfect Because the matrix that we use and its
recall and a non-normalized discounted cumulative gain you see more in the papers that I showed you They are
disconnected now for the downstream answer quality So the real answer the
end answer for the human query is not integrated here in the performance
benchmark And you might say h maybe this is something that I want Yeah because
the end product if you want is the quality of the final answer given here
by the artificial intelligence So we had a new publication and this is here also University of Illinois University of
Massachusetts and Google Cloud AI research search R1 So you see what Deep
Seek and R1 in particular ignited is much beyond what you maybe thought
because it has a massive impact here on reinforcement learning and on rack
system This paper here was beautiful Search or one trained here a single
model a language model to jointly retrieve and generate here via reinforcement learning using now here as
a reward function here the exact match While this approach now improved
for the accuracy the tight entanglement between the search and the generation makes it
difficult to isolate here some real genuine retrieval improvements So and
they did this by the way for for the PO for the classical PO with a search engine and for the group relative policy
optimization with a search engine And you remember here we do not have here the value LLM we go without We have a
relative group specific Of course we have a cool lab divergence but what they
did and you have here the advantage function Beautiful We talked about this here for the group computation And of
course during the roll out LLMs can conduct multi-turn interaction with the
search engine So you don't have here a single turn but you go now in multi-turn So this is really an optimization that
we are looking for And this was really a very important paper with some great
performance improvements because the LLM learns now and later the agent to autonomously generate multiple search
queries during the step-by-step reasoning with the realtime retrieval So
you learn on the real available data volume on the data quality of this
particular moment in time and you don't have to reference some to some data
quality maybe a month back maybe a week back or whatever So you have a real time
optimization problem that you can solve benchmark search all one improved
the performance by 41% if you go with a Q1 2.57 billion free trainable parameter and at least if
you go with a 3B 20% over the rack baseline under the same settings so real
impressive performance gains here so great yeah if you want to read this in
more details I have given you the text we have here simply the search here we
have the designated call token Then search and end of search Then retrieve the information We have the information
and end of information special tokens and the answer tokens Have a look at this This is a
beautiful GitHub search one and it's for me it serves another purpose like an
open solution here to open deep research enabling here research and development in a tool augmented LLM reasoning
If you have a look at this you immediately notice here how is it working on what is it
working and it is working here and I have to do a video on this on a volcan volcano engine reinforcement learning
for LLMs This seems to really be now the new standard
It's a production ready reinforcement learning training library for
LLMs and more or less every single AI publication I've seen in the last month
here especially here from Asia Southeast Asia and China and those uh expert
location they run here with our volcano engine reinforcement learning if you want to know I have I'm
planning a video on hybrid flow now for more than two weeks already But yeah I have to do it
somehow If you want this is here the open version of the hybrid flow paper but this is a little bit more
challenging paper So I have to do a specific video on this But just if you want to know if you want to run this
operational get yourself familiar at least here with the Volcano engine or if
you're really into the theoretical deep dive with hybrid flow I have to do a video on this So what we achieved llams
learn through direct trial and error Great because we have a real-time data
connection We have real-time data and we have a real to the minute training of
the system with the real data We have no synthetic data We have no simulation We
are not referring to data that are maybe months old You find these models on hugging face Great
However if you're really into the details and if you are say hey I am here
just for make perfect you notice that search or one
trains here a single language model to jointly retrieve and generate using here of course the exact match here as the
reward function but as I told you entangles here search and generation it requires here the full model tuning and
here particular this metric is a little bit brittle so oh not the optimal on So
we have to go a step further Just a summary if you're not really
familiar again rack reinforcement learning PO search agent just for you to
summarize and now let's come to a new paper and now we say listen I have an
idea what about you reward now the assistant LLM or the agent based on how
much better the expert LLM or agent performs with the information the assistant agent provides You compare how
much the expert would have performed with the basic naive rag and you develop
a complete new metric and you call it here the gain beyond the naive rag
system and it's called GBR okay so what we have think about
this we have now at least two or three agents or LLM expert LLM that are really
trained on a particular domain for a really downstream task We have this one and of course sometimes
your open EI you have not access to the model you can't fine-tune it you can't or you can do nothing this is
appropriate model and you have to pay them that they do it for you so this is not perfect but you say no problem
because I have other agents here in my cluster and if you now optimize here the
search LLM this will do a lot of the work for you and you can get some significant
performance increases So have let's have a look at this So by focusing now the learning signal if you want directly on
the value added by the complete search process to a now fixed generator like an
openi model that I cannot have access to the weights We have now a new method
It's called S3 And this efficiently trains now a highly effective search LLM
or search agent for rack with our fixed generator
LLM I achieve a modularity This means that the train searcher LLM can serve
now any capable generator LLM So if you go now with I don't know openi if you go
with entropic with claude whatever it doesn't matter because the work is done
here on the interface here on the optimization of the compound but
focusing on the search agent Now it decouples the search agent or
multiple search agent depending on the complexity of your system from the generation LLM from openi3 pro or
whatever you have Then it optimizes here the search agent retrieval strategy based on its direct impact on the
downstream generation task performance It is therefore compatible with any
frozen or blackbox LLM from OpenI And we achieve a real strong performance gain
with significant and this is now amazing with less training data than all our
other based reinforcement learning based rack methodologies This is nice Yeah S3
stands for optimized search select and surf It's a lightweight model agnostic
framework that decouples here the search agent from the generator agent So it trains here searcher LLM using
reinforcement learning classical what we learned from the deepse R1 model with a novel reward signal functionality called
gain beyond rack Now of course you immediately understand that it is not
just a classical naive rack No but you can have a graph rack you can have a split drag whatever you have currently
operational in your system You go now and say "Hey this is the the the worst
possible case and you look for a gain beyond this what you have implemented
and you optimize it against your current best benchmark." I think it's such a
beautiful idea Look this is yet a publication Yes I know it's not from yesterday but the day before yesterday
May 20 but never mind S3 University of Illinois and
Amazon you don't need that much data to train a search agent via reinforcement
learning And if you look now at the performance data you see here the average score so up is better So S3 here
the red one is real nice in the general domain rack Or if we go here of medical
domain rack you here one of the best performing indicator here Plus you have
here on the x-axis here the amount of training data that you need from 2.4K to
170K But look where S3 is located here This is beautiful So you have a
little bit more of this search or one methodology of a 7B pre-trainable
parameter model but with much less training data This means it cost much
less less time cost less money and you have the same performance This is really
nice for medical it's a little bit special You see here if you have your deep retrieval here you are a little bit
better here and the quality but the amount of training data that you need is simply
amazing They tell us here it's about compared to baseline the other system
need about 70 times more data and this S3 consistently deliver strong download
per downstream performance across six general Q&A and five medical Q&A
benchmarks So you see we have and this is also the way I build up this video till now We have here a classical rack
the retrieve and the generation And then we have here the pre-reinforcement the
pre-deseek RL0 method here where we had a distillation a supervised fine-tuning
This is what we call self rag I have a particular video on self rag It's more than a year old And then with deepsek
reinforcement learning zero we have now here all this deep retrieval search all
and now the brand new S3 the metric gained beyond the classical rag as a
reward signal coming back in the optimization loop for training your
model Real nice So just to make sure here from the original paper you have
here this visualization you see this searcher S3 ends really here it is has
here a clear metric and it does nothing to the generator the openi model that
does now here if you want here the whatever because this is frozen we have
no access to the openi propriatory model so therefore to make it here so clever
this model system here of the latest drag implementation You can use any
generator LLM open source proprietary whatever big small compressed quantized
as you like and you will get a good performance And now the main question
that remains for me is but wait if I use here an end to end approach like in
search R1 Okay I have the same model but I have a complete system
optimization If I go now only with a search optimization would I be at least
as powerful in my performance as would have a complete generation accuracy uh
metric defined for the performance increase So how much better is this gain
beyond rack system Now if you go here with
naivex where are the data Yeah I just want to show you this here The cycle as
I told you data are coming in a second Just give me a sec This gain beyond drag
It's so beautiful because it's a relative metric that we have here Our
optimization you have your human question and then you have your search agent Let's call it the searcher Great
The searcher has now access we told use to let's say the Google search engine
somewhere or your particular search engine Never mind So we have now
documents coming back doc one doc two whatever we have information coming back as a feedback from our let's say
internet query and then we have the doc ids and then we have here the planning and we
say hey is the search complete have we achieved here what we set out to do our metric our parameter really do we have
now achieved what we wanted if it's a no you have a query generation and you
start here a complete loop all over again if we have everything that we need
Great So all important docs here from the S3 model Great What we have now Yes
of course we can then go and hand over the results here to a generator LLM that
is frozen like Open AI But we also can compute now the particular reward function because we
want to be better than anything else And here if you see here this is just
here where you go here with your DS3 minus here your D naive rack
coefficient You can go here with any other rack system that you want You
optimize the system against a very high performance benchmark and you let just
the system self-train itself to become better as you already whatever you
already have And I think this is an absolute fascinating aspect here and
it's really a gain beyond whatever rack system you have implemented reward
function and you have a reinforcement learning training with this particular metric because this now updates here the
search agent and for your particular query and for your particular search
engine for your domain specific data for your complexity level that you are searching you can now run this because
maybe if you have a real simple search then if rag will do the job you don't
need to do any crazy stuff yeah but if you have some real complex problems then
this system will optimize itself and I think this is just beautiful the search element interacts
with a search engine iteratively generates queries retrieves not the documents like here selects a subset of
useful evidence great and decides whether to continue searching yes or
And this as I told you here this is so beautiful This effectively filters out question that are solvable by naive rag
allowing it here to focus really here on the harder queries where improved retrieval is essential for generation
success What a beautiful thing But you know what If you look now at the real
code implementation you see and this is a screenshot here from their publication
they still go with PO and I understand it regarding now the
stability of the system but hey come on PO
2017 I mean just look at my videos here I said hey forget here PO we have DPO
and then we had GRPO and here the videos from GPO to DAPO to VAPO or you say hey
what about here complete new generative reward models from Deep Seek where we have a self-principle critique tuning or
if you want to go with uh reinforcement learning go here with zero reinforcement learning and off policy Luffy where I
showed you here complete new methodologies because you remember this is more than a year old we had here my
god entropy can you remember prompt caching here re-ranking methodologies
this is really stone age and then we had from Oxford here agentic roar the update
And then we had graph rag and light rag and prag and we went further to multi-
aent G rags with knowledge graph and agent here for our rack system And then
we had here a demo with GBD4.1 where I showed you the security breach with a chain of defensive sort
rack system So it is amazing what we already have So just giving this new
idea and let's say it's PO implementation My goodness I have 10
trajectory how we can optimize the system even further But let's stay with the publication by the team So we are
back now here to S3 and they tell us here the orers hey we have three main takeaway and as I told you hey how is it
now performing against an end toend optimization and this is amazing and
this is really great because they tell us the searcher only agent is better
than the end to end optimization for rack I did not expect that system to be
true So they tell us S3 consistently outperforms the search R1 on search
quality revealing that most of the performance gain in rack system stem here from improving the search
capability instead of aligning here the generation outputs They tell us here the searcher
only training enables also a domain transfer functionality plus the reward choice
itself directly shapes here the search quality seems yes of course but think
about this for a secondh so using sematically or human preference aligned metric like whatever you have an
accuracy function encourages here the search policy to retrieve substantially helpful documents rather than just
optimizing here for some brittle string overlap in your documents And you know at the end this
are the conclusion by the authors We give here the the scene here to the authors and they tell us our results
show that a targeted search policy learning yields substantial gains in
both the efficiency and the generalization offering here scalable power for improving here the rack system
even further You noticed here it's about the search
policy learning about the interaction with the search engine and if we talk
about search engine comes to mind Google so Google is here in a perfect
place to have here further optimized rack system because if it's really true
what the authors of this S3 publication tell us it is not about the general
optimization but it is really that you have the perfect search engine that brings you back the data that you need
for the user query and that the search policy learning is the most important
thing here and the complete interconnected system then I think open who does not
have maybe a search engine that is as powerful as Google's Google is a real
great place here talking about Google I found here IO25 Five quite disappointing
because they were talking about products about new offering to their client There
was not a lot about real hardcore research on AI but as you see yeah for
research here on AI you have to come to my channel and Google AI was more about product placement and new innovation
that they wanted to introduce you to the broader public Now I told you that there are
quite a lot of rag is really exploding here in the last days and I want to give you your feeling So this is here May
21st and we have year for Chinese University of Hong Kong and Tenzent RGA finetuning you know the age-old question
here comparative studies on LCM based code completion now in industrial where
is your retrieval augmented generation what should you do should you rely here for your code optimization on a rag
system or should you go and say hey I have here my training data and I do fine-tuning on my LLM or on my code
generator because this is more efficient I do not trust rag to find here really the
perfect code for my per for my query because maybe rack does not understand
exactly what code sequence I need Then same day we have here National University of Singapore picking
university and chinua university implicit knowledge extraction attack on rack systems through benign queries We
have here what I like here is the is the shortcut EK I love this It's about
knowledge extraction on rack system and they give you here some great ideas Also May 21st here picking
university They have here scalable defense You see it's a lot about cyber security against in the wild
jailbreaking attacks here with safety context retrieval especially for rack techniques really interested if you're
into cyber technology or same day May 21st we have here Renmmin University of
China and Beijing Academy of Artificial Intelligence single LLM multiple roles
unified retrieval augmented generation framework using ROP specific token
optimization within the handling of subtask within the rack process itself
Real absolutely fascinating here They propose a ro rag system for unified rack
framework to achieve efficient multitask processing through ro specific token optimization Have a look at this This is
really a study I I selected for you I really recommend that you have a look or if you really now this is a hardcore for
if you want uh May 21st 2025 now for if
you want hardware design task Yeah this is it So we have here what we working with a graph retrieval augmented
generation So we have a graph rack system but we are working here on hardware optimization and hardware
description language generation and language debugging functionalities here
So also some brand new ideas with some integrated drag optimization here in the
relevant fragments High quality whatever and if you're thinking this was a particular day no the day before this So
yesterday for me on May 20 we had also here the semantic document layout
analysis for textual and visual rack system retrieval augmented generation from neck corporation or you go here
with aluring institute in King's College London automatic data set generation for knowledge intensive question answering
task here where rack continues to face challenging and handling complex reasoning and logical connection between
multiple sources of information and They want to generate here data set to further improve the performance of rack
or you go here with Alberg University and the technical university of Vienna
Hello Vienna with multilingual data set for knowledge graph grounded evaluation of LLM hallucinations here So they work
in the interplay of a knowledge graph with Rex system Absolutely fascinating Also real nice Kyoto
University University of Tokyo and Reichen beyond change bridging LLMs and
knowledge bases in complex question answering We have here the chainbased knowledge graph rack methodology that
they try to further develop And they have here a four-stage framework for predict decompose retrieve and reason
Real nice idea You have to have a look at this publication if you want to see what's currently happening in rag and
yeah it goes on city university of Hong Kong and new arc lab who are buy in
China and they tell us here process versus outcome reward the reward function especially here in our multi-
aent rag systems are so important if you go with a real outcome reward but more
and more we understand that the process reward has so much more to offer and they ask you which is better for when a
giant grab act reinforcement learning So this is really on topic what we talk today and they have here an absolute
they develop here a novel method they call here a reason rack system that
automatically construct your high quality data set for evaluation So beautiful rack currently is exploding
have a look at this rack in the classical sense for I don't know one or two years it's not there anymore We've
moved here completely to agentic system but rack is still in development under
development We have brand new ideas We completely have a redesign of rack here
and the performance increases Amazing Next one is here Australia Yeah this is
what I wanted to show University of New South Wales Sydney Australia Divide by question by agent a split rag This is
what I showed you at the very beginning of my video with a question to graph partitioning here a split rack multi-
aent rack framework So absolutely fascinating to create here semantic partitioning of link information with a
type specialist knowledge base But I think I I hope you got the idea Rack
currently is having a huge revival We have so many brand new ideas I think
everything started more or less with Deep Seek all one This this was such an important model such an important paper
It showed us new development uh uh trajectories here for rag Rag was almost
dead before Deepse 1 And now I can tell you rag absolutely amazing Have a look
this for yourself Rack explained I hope if you are a newcomer to rag and you just want to have idea where's rag today
I hope you enjoyed it you had a little bit of fun And if you enjoy these kind of videos hey why not subscribe And I
see you in my next one
